\documentclass{article}

\usepackage{graphicx,tikz}
\usepackage{fullpage}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
%\usepackage{booktabs}
%\usepackage{color}

%\renewcommand{\baselinestretch}{0.95}
%\setlength{\textfloatsep}{0.1cm}
%\setlength{\abovecaptionskip}{0.1cm}

\begin{document}

\title{\Large\bf An Asymptotically Tighter Bound on Sampling for Frequent Itemsets Mining}
\author{Shiyu Ji\\ shiyu@cs.ucsb.edu}
\date{}
\maketitle

\newtheorem{definition}{Definition}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\theoremstyle{plain}
\newtheorem{lemma}{Lemma}
\theoremstyle{plain}
\newtheorem{corollary}{Corollary}
\theoremstyle{plain}

\begin{abstract}
In this paper we present a new error bound on sampling algorithms for frequent itemsets mining. We show that the new bound is asymptotically tighter than the state-of-art bounds, i.e., given the chosen samples, for small enough error probability, the new error bound is roughly half of the existing bounds. We also give a new approximation algorithm, which is much simpler compared to the existing approximation algorithms, but can also guarantee the worst approximation error with precomputed sample size.
\end{abstract}

\section{Introduction}
Frequent Itemsets (FI) mining has been popular in research recently \cite{AIS93, HCX07, RU15}. The goal of FI mining is to find out the items that most frequently appear in the observed transactions, e.g., the researchers who are the most prolific in writing papers with others, the patterns that appear frequently in a long piece of genetic code, etc. 

In the era of big data, to compute the exact frequencies can be very time consuming. Thus in many cases approximated values are also acceptable \cite{RU15}.
For FI mining in large scaled transactional dataset, we often take samplings on the transactions, and compute the frequencies of the itemsets among the sampled transactions as approximated results of their true frequencies among all the transactions. Usually the sampling size is much less than the scale of all the transactions, and the approximations can achieve acceptable precision. Thus FI approximation can be useful in practice.

The state-of-art progressive sampling based FI approximation algorithms \cite{RU15} need an upper bound of the approximation error for the worst case, i.e., the maximum error the algorithm can generate among all the items. The algorithms keep taking new samples until the upper bound is less than the acceptable threshold. Hence how to bound the maximum error as tightly as possible is an interesting problem. The current bounds use some results of Rademacher average in statistical learning theory \cite{Vap98,Vap13,BBL04,BBL05}. 
However, we find that based on the idea given by \cite{BBL04}, we can develop a new upper bound \emph{without} Rademacher average. 
We also find that this new bound is asymptotically tighter than the existing bounds, i.e., given the chosen samples, as the allowed error probability approaches zero, the new bound is roughly only half of the existing ones. This implies that by using the new bound, a progressive sampling based FI approximation algorithm can reach the guaranteed accuracy with much fewer samples. We also notice that there is no parameter in the new bound that needs to be progressively computed. Hence the sample size that will guarantee the worst error can be precomputed.

Based on the similar idea, we also consider the top-$k$ FI mining problem, which seeks for the $k$ most frequent itemsets in the observed ones. We need to decide when the sampling should stop. The number of the samples transactions is enough if the worst-case error upper bound is less than the frequency gap between the $k$-th and the $(k+1)$-th most frequent itemsets. Hence we propose a progressive approximation algorithm to address the top-$k$ FI mining problem.

{\bf Our Contributions}. We give a new worst-case error upper bound that is asymptotically tighter than the state-of-art bounds, and propose an approximation algorithm which can guarantee the worst-case error upper bound with precomputed sample size. We also give a progressive sampling algorithm to find the top-$k$ most frequent itemsets.

The rest of this paper is organized as follows. Section \ref{sec:rw} reviews the related research works. Section \ref{sec:prlm} introduces the notations and preliminaries throughout this paper. Section \ref{sec:refine} gives our new worst-case error upper bounds and compares them with the existing ones. Section \ref{sec:algs} proposes our approximation algorithms based on our upper bounds.

\section{Related Works}
\label{sec:rw}
Sampling-based frequent itemset approximation has been studied extensively by the researchers. The first works on this problem used heuristic methods to progressively approximate the frequencies \cite{CHS02,CCY05,Parth02}. There were no guarantee on the worst-case error upper bound. To fix this, Riondato and Upfal were the first to propose FI approximation algorithms that could guarantee the worst error bounds by using results of Vapnik-Chervonenkis (VC) dimension \cite{RU12,RU14} and Rademacher average \cite{RU15}. Note that in statistical learning theory VC dimension and Rademacher average are usually used to address the worst-case error upper bound for \emph{infinite case}, i.e., the number of possible functions in the learning model is infinite \cite{BBL04}. However in the case of FI mining, since there are only \emph{finite} itemsets, it is possible to develop bounds without VC dimension or Rademacher average \cite{BBL04}. In this paper we apply this idea on FI mining problem. Riondato et al. also considered using parallelism in FI mining \cite{RDF12}, which is an orthogonal topic to sampling-based FI approximation.

In practice we are often only interested in the most frequent itemsets. Thus top-$k$ FI mining is a popular research topic with many research works \cite{PRU10,SW02,RU15,RV14}. 

\section{Preliminaries}
\label{sec:prlm}
\subsection{Frequency of Itemset}
\newcommand{\I}{\mathcal{I}}
\newcommand{\D}{\mathcal{D}}
In this paper we use the notations and definitions from Riondato and Upfal's pioneering work \cite{RU15}. 
Let $\I$ be the set of items. A transaction $\tau$ is a subset of $\I$ (i.e., $\tau \subseteq \I$).
An itemset $A$ is a set of items that appear together in a transaction $\tau$, i.e., $A \subseteq \tau$. Clearly any itemset is also a subset of $\I$. 
Let transactional dataset $\D$ be the set of all the transactions. In this paper we always assume $\D$ is a finite set. Denote by $T_\D(A)$ the set of all the transactions in $\D$ that contain the itemset $A$. $T_\D(A)$ is also known as the support set of $A$ in $\D$.
If $\D$ is a finite set, we can define the frequency of itemset $A$ in $\D$ as the fraction of transactions in $\D$ that contain $A$.
$$f_\D(A) = |T_\D(A)|/|\D|.$$
Clearly $0 \leq f_\D(A) \leq 1$ for any $A \subseteq \I$.

The goal of our sampling algorithm is to approximate $f_\D(A)$ given an itemset $A$ as accurately as possible.

\subsection{Approximation Algorithms}
\newcommand{\Smp}{\mathcal{S}}
An $(\epsilon,\delta)$-approximation algorithm of the frequencies $f_\D(\cdot)$ takes as input all the items $\I$ and outputs a sampled average $f_\Smp(A)$ for each $A\subseteq\I$ such that with probability at least $1-\delta$,
$$\max_{A\subseteq\I}|f_\D(A) - f_\Smp(A)| \leq \epsilon.$$
We often use progressive sampling \cite{RU15,RU16}, i.e., to keep taking more samples until a stopping condition is reached. A stopping condition usually takes the form $\Delta(n, \delta) \leq \epsilon$, where $n$ is the number of samples that have been taken, and $\Delta$ is an upper bound of the worst approximation error given by statistical learning theory. Note that $\Delta$ is usually a function of $n$ and $\delta$.

There is a variant called top-$k$ approximation, which returns the $k$ most frequent itemsets among the observed ones based on the approximated frequencies. This is quite popular in practice since we are often only interested in the most common itemsets. 

\subsection{Risk Bounds}
\label{sec:rb}
\newcommand{\R}{\mathcal{R}}
We briefly review some risk bounds in statistical learning theory \cite{BBL05} with the background of frequent itemsets mining. 

For each itemset $A\subseteq\I$, define the indicator function $\phi_A : 2^\I \to \{0, 1\}$ as follows.
$$\phi_A(\tau) = \begin{cases}
1 & \textrm{if $A\subseteq \tau$}\\
0 & \textrm{otherwise}\\
\end{cases},\quad
\tau\subseteq\I.$$
Clearly, the frequency $f_\D(A)$ equals to the \emph{true} average of $\phi_A(\tau)$ where $\tau$ goes over all the transactions in $\D$.
$$f_\D(A) = \frac{1}{|\D|} \sum_{\tau\in\D} \phi_A(\tau).$$
Similarly let $\Smp$ be the set of the sampled transactions. Then the \emph{sampled} average of $\phi_A(\tau)$ can be defined as
$$f_\Smp(A) = \frac{1}{|\Smp|} \sum_{\tau\in\Smp} \phi_A(\tau).$$
Clearly $f_\Smp(A)$ is the frequency of $A$ appearing in the sampled transactions $\Smp$.

Assume $|\Smp| = n$. For each transaction $\tau_i \in \Smp$, let $\sigma_i$ be a Rademacher random variable taking value from $\{-1, 1\}$ with uniform probability distribution. The $\sigma_i$'s are independent. Assuming $\I$ is finite, we define the sample conditional Rademacher average as follows.
$$\R_\Smp = \mathbb{E}_\sigma \left[\max_{A\subseteq\I}\frac{1}{n}\sum_{i=1}^n \sigma_i\phi_A(\tau_i)\right],$$
where $\mathbb{E}_\sigma$ denotes the expectation taken over all the random variables $\sigma_i$'s, conditionally on the sample $\Smp$. 

The following theorem tells us that Rademacher average can be used to upper bound the approximation error, even for the worst case.

\begin{theorem}
\label{thm:old}
(Theorem 3.2, \cite{BBL05}) For any $\delta>0$, with probability at least $1-\delta$,
$$\max_{A\subseteq\I} |f_\D(A) - f_\Smp(A)|\leq 2\R_\Smp + \sqrt{\frac{2\log(2/\delta)}{n}}.$$
\end{theorem}

If we want to use the upper bound given in Theorem \ref{thm:old} in an approximation algorithm, we still need to upper bound the $\R_\Smp$. A classical result is given by Massart \cite{Mas00}.

\begin{theorem}
\label{thm:massart}
(Lemma 5.2, \cite{Mas00}) Let $\ell = \max_{A\subseteq\I} [\sum_{i=1}^n\phi_A(\tau_i)^2]^{1/2}$ where each $\tau_i\in\Smp$. Then
$$\R_\Smp \leq \frac{\ell}{n}\sqrt{2\log N},$$
where $N = 2^{|\I|}$ and $n = |\Smp|$.
\end{theorem}
Hence we have the following stopping condition for an $(\epsilon,\delta)$-approximation sampling algorithm.
$$\Delta_1 := \frac{2\ell}{n}\sqrt{2\log N} + \sqrt{\frac{2\log(2/\delta)}{n}} \leq \epsilon.$$
However for many applications the above bound is not tight enough \cite{RU15,RU16}. In the next section we will first review the state-of-art bound on the worst approximation error, and then propose a new bound which seems tighter.

\section{Refining the Upper Bound}
\label{sec:refine}
The reason why the bound given in the previous section is often not tight enough in practice is that the $\ell$ defined in Theorem \ref{thm:massart} can be quite large. Suppose there is an itemset $A$ that almost always appears in every transaction in $\D$. Then no matter which sample the algorithm chooses, $\ell$ is roughly $\sqrt{n}$. For $\delta=0.01$, $N = 2^{1000}$, even 100,000 samples are taken, the upper bound is still larger than 0.15. For many applications such an upper bound cannot be acceptable and thus we need to take more samples. Clearly if the upper bound is tighter, a lot of samples can be saved.

\subsection{A Brief Review on the Existing Results}
Riondato and Upfal \cite{RU15} attempted to give a tighter bound of the Rademacher average $\R_\Smp$. 

\begin{theorem}
\label{thm:ru}
(Theorem 3, \cite{RU15}, revised) Let $w : \mathbb{R}^+ \to \mathbb{R}^+$ be the function defined as
$$w(s) = \frac{1}{s}\log \sum_{A\subseteq\I}\exp\left(\frac{s^2 \sum_{i=1}^n \phi_A(\tau_i)^2}{2n^2}\right).$$
Then $\R_\Smp \leq \min_{s>0} w(s)$.
\end{theorem}

{\bf Remark}. Note that in Theorem \ref{thm:ru}, the summation in $w(s)$ takes \emph{exactly} $2^{|\I|}$ terms. However in the original version in \cite{RU15}, the authors claimed that the summation could take much less than $2^{|\I|}$ terms. We argue that there is a gap between these two versions. Based on the proof given in \cite{RU15}, one can reach the inequality as follows.
\begin{equation}
\label{eqn:ru}\exp(s\R_\Smp) \leq \sum_{A\subseteq\I}\exp\left(\frac{s^2\sum_{i=1}^n \phi_A(\tau_i)^2}{2n^2}\right).
\end{equation}
Note that on the right hand side, each term in the summation is no less than 1. Hence when taking the logarithm on both sides and dividing by $s$, each of the $2^{|\I|}$ terms cannot be eliminated. Thus the range of the summation cannot be compressed.

Formally, suppose there is a set $\mathcal{V}\subseteq 2^\I$, where $2^\I$ denotes the power set of $\I$, such that
$$\alpha(s) := \sum_{A\in 2^\I}\exp\left(\frac{s^2\sum_{i=1}^n \phi_A(\tau_i)^2}{2n^2}\right) \leq \sum_{A\in \mathcal{V}}\exp\left(\frac{s^2\sum_{i=1}^n \phi_A(\tau_i)^2}{2n^2}\right) :=\beta(s).$$
We take the limits as $s$ approaches 0.
$$2^{|\I|}=\lim_{s\to 0}\alpha(s) \leq \lim_{s\to 0}\beta(s) = |\mathcal{V}|.$$
Hence $\mathcal{V} = 2^\I$. This implies any summation over only a part of $2^\I$ must be less than the summation over all of $2^\I$. Thus one cannot use Inequality (\ref{eqn:ru}) to reach Theorem 3 in \cite{RU15}.

\subsection{New Bound Without Rademacher Average}
In statistical learning theory, the upper bound given by Theorem \ref{thm:old} is for the general case, i.e., the set of transactions $\D$ can be infinite or finite. However, for frequent itemsets mining, the transactional data set $\D$ is always finite. Given the assumption that $\D$ is finite, can we establish any upper bound without using the Rademacher average? Following the similar lines given by Boucheron, Bousquet and Lugosi \cite{BBL04}, we can give an affirmative answer.

For any $\epsilon > 0$,
$$\begin{aligned}
& \Pr[\max_{A\subseteq\I}|f_\D(A)-f_\Smp(A)|>\epsilon] \\
= & \Pr[\exists A\subseteq\I, f_\D(A)-f_\Smp(A)>\epsilon \vee f_\D(A)-f_\Smp(A)<-\epsilon] \\
\leq & \Pr[\exists A\subseteq\I, f_\D(A)-f_\Smp(A)>\epsilon] +\Pr[\exists A\subseteq\I, f_\D(A)-f_\Smp(A)<-\epsilon] && \textrm{(union bound)}\\
\leq & \sum_{A\subseteq\I} \Pr[f_\D(A)-f_\Smp(A)>\epsilon] + \sum_{A\subseteq\I} \Pr[f_\Smp(A)-f_\D(A)>\epsilon] && \textrm{(union bound)}.
\end{aligned}$$
Recall Hoeffding's inequalities \cite{H63}. Let $X_1, \cdots, X_n$ be independent random variables bounded by the intervals $[a_i, b_i]$. Define the sampled average of them as
$$\overline{X} = \frac{1}{n}\sum_{i=1}^n X_i.$$
Then for any $t>0$,
$$\Pr[\overline{X} - \mathbb{E}[\overline{X}] > t] \leq \exp\left(-\frac{2n^2t^2}{\sum_{i=1}^n (b_i-a_i)^2}\right),$$
and
$$\Pr[\mathbb{E}[\overline{X}] - \overline{X} > t] \leq \exp\left(-\frac{2n^2t^2}{\sum_{i=1}^n (b_i-a_i)^2}\right).$$
Note that if we set $X_i = \phi_A(\tau_i)$, then $X_i$'s are independent since $\tau_i$'s are independent, and thus $f_\D(A) = \mathbb{E}[\overline{X}]$ and $f_\Smp(A) = \overline{X}$. Based on Hoeffding's inequalities, 
$$\Pr[f_\D(A) - f_\Smp(A) > \epsilon] \leq \exp\left(-2n\epsilon^2\right),$$
and
$$\Pr[f_\Smp(A) - f_\D(A) > \epsilon] \leq \exp\left(-2n\epsilon^2\right).$$
Putting the above results together, we have
$$\begin{aligned}
& \Pr[\max_{A\subseteq\I}|f_\D(A)-f_\Smp(A)|>\epsilon] \\
\leq & 2\sum_{A\subseteq\I} \exp\left(-2n\epsilon^2\right) \\
= & 2N \exp\left(-2n\epsilon^2\right),
\end{aligned}$$
where $N = 2^{|\I|}$. 
Equivalently for any $\delta>0$, with probability at least $1-\delta$,
$$|f_\D(A)-f_\Smp(A)| \leq \sqrt{\frac{\log(2N) + \log(1/\delta)}{2n}} =: \Delta_2.$$
Note that the above bound $\Delta_2$ is very similar to the result in Section 3.4, \cite{BBL04}. Now the new bound $\Delta_2$ can generate a new stopping condition for an approximation algorithm.

Recall the classical upper bound given in Section \ref{sec:rb}.
$$\Delta_1 := \frac{2\ell}{n}\sqrt{2\log N} + \sqrt{\frac{2\log(2/\delta)}{n}}.$$
Clearly $\lim_{\delta\to 0}\Delta_1/\Delta_2 = 2$, i.e., when $\delta$ is very small, the bound $\Delta_1$ is roughly twice of $\Delta_2$ given the sample size $n$.
This assures us that the new bound $\Delta_2$ is highly competitive.

Theorem \ref{thm:ru} can give another upper bound on the worst approximation error. However, since the number of terms in the summation grows exponentially on $|\I|$, to find the minimum is computationally infeasible. Furthermore, even if the minimum $w(s^*)$ is found, let $\Delta_1'$ be the upper bound of this variant defined as 
$$\Delta_1' := w(s^*) + \sqrt{\frac{2\log(2/\delta)}{n}}.$$
By fixing the sample $\Smp$, we still have $\lim_{\delta\to 0}\Delta_1'/\Delta_2 = 2$. For small $\delta$, the new bound without Rademacher average still outperforms the existing ones.

\section{Our Frequent Itemset Approximation Algortihm}
\label{sec:algs}
\subsection{Approximating with Precomputed Sample Size}
We observe the new bound given in the previous section:
$$\Delta_2 := \sqrt{\frac{\log(2N) + \log(1/\delta)}{2n}},$$
where $N = 2^{|\I|}$.
The upper bound $\Delta_2$ can be treated as a function of allowed error probability $\delta$, sample size $n$ and $N = 2^{|\I|}$, all of which are already given. A good news is that there is no parameter that needs to be progressively computed (e.g., $\ell$ in $\Delta_1$). Thus to guarantee an worst approximation error at most $\epsilon$, we only need to make sure $\Delta_2 \leq \epsilon$. By solving it we have
$$n \geq \frac{1}{2}(\log 2N + \log (1/\delta)).$$
Hence an $(\epsilon,\delta)$-approximation algorithm takes a very simple form.

\framebox{
\begin{minipage}{.9\textwidth}
\underline{\bf Frequent Itemsets Approximation Algorithm}

{\bf Input}: items $\I$, transactional dataset $\D\subseteq 2^\I$, $\epsilon>0$, $\delta>0$.

{\bf Output}: approximated frequencies $\hat{f}_\D(A)$ for each $A\subseteq\I$ s.t. with probability at least $1-\delta$, $|\hat{f}_\D(A)|\leq \epsilon$ for any $A\subseteq\I$.

\begin{enumerate}
\item $n \gets \lceil\frac{1}{2}(\log(2^{|\I|+1}) + \log (1/\delta))\rceil$.
\item $\Smp \gets \emptyset$.
\item If $n\geq |\D|$, $\Smp \gets \D$; otherwise, choose $n$ itemsets in $\D$ at uniformly random and add them to $\Smp$.
\item Label the transactions in $\Smp$: $\Smp = \{\tau_1,\cdots,\tau_n\}$.
\item For each $A\subseteq\I$, compute $\hat{f}_\D(A) \gets \frac{1}{n}\sum_{i=1}^n \phi_A(\tau_i)$.
\item Return all the $\hat{f}_\D(A)$'s for $A\subseteq\I$.
\end{enumerate}
\end{minipage}}

Note that the algorithm above estimates the frequency for every subset in $\I$. Hence it is inefficient when $|\I|$ is large. In practice, we often only care about the frequencies of a few itemsets, e.g., celebrities in a community, influential papers in a field, etc. For this case, we do not have to estimate the frequencies of the rest itemsets. 
\newcommand{\Ob}{\mathbf{Ob}}
Denote by $\Ob$ the set of the itemsets to be observed. Then the worst approximation error is defined as the maximum error on every itemset in $\Ob$. By the same reasoning in the derivation of $\Delta_2$, we have the adjusted new bound:
$$\Delta_2' := \sqrt{\frac{\log(2|\Ob|) + \log(1/\delta)}{2n}}.$$
Note that since $\Ob$ is a subset of $2^{\I}$, this bound $\Delta_2'$ is tighter than $\Delta_2$. The approximation algorithm will also be revised as follows.

\framebox{
\begin{minipage}{.9\textwidth}
\underline{\bf Frequent Itemsets Approximation Algorithm with Observed Items}

{\bf Input}: all the items $\I$, the observed items $\Ob\subseteq 2^\I$, transactional dataset $\D\subseteq 2^\I$, $\epsilon>0$, $\delta>0$.

{\bf Output}: approximated frequencies $\hat{f}_\D(A)$ for each $A\in\Ob$ s.t. with probability at least $1-\delta$, $|\hat{f}_\D(A)|\leq \epsilon$ for any $A\in\Ob$.

\begin{enumerate}
\item $n \gets \lceil\frac{1}{2}(\log(2|\Ob|) + \log (1/\delta))\rceil$.
\item $\Smp \gets \emptyset$.
\item If $n\geq |\D|$, $\Smp \gets \D$; otherwise, choose $n$ itemsets in $\D$ at uniformly random and add them to $\Smp$.
\item Label the transactions in $\Smp$: $\Smp = \{\tau_1,\cdots,\tau_n\}$.
\item For each $A\in\Ob$, compute $\hat{f}_\D(A) \gets \frac{1}{n}\sum_{i=1}^n \phi_A(\tau_i)$.
\item Return all the $\hat{f}_\D(A)$'s for $A\in\Ob$.
\end{enumerate}
\end{minipage}}

Note that we do not have to estimate for any itemset which is out of the observed ones $\Ob$.

\subsection{Approximating Top-$k$ Frequent Itemsets}
In practice we often need to find out the top-$k$ frequent itemsets among the given candidates $\Ob$. We can slightly revise the algorithm given in the previous section to approximate the $k$ most frequent itemsets. A new problem here is how to determine the approximation error $\epsilon$, which should be less than the distance between the $k$-th and $(k+1)$-th most frequent itemsets. It is also possible that these two itemsets have the same frequency. Combining these cases, a progressive sampling approximation algorithm can be given as follows:

\framebox{
\begin{minipage}{.9\textwidth}
\underline{\bf Top-$k$ Frequent Itemsets Approximation Algorithm with Observed Items}

{\bf Input}: all the items $\I$, the observed items $\Ob\subseteq 2^\I$, transactional dataset $\D\subseteq 2^\I$, sampling increase $\Delta n$, $k>0$, $\delta>0$.

{\bf Output}: $k$ itemsets among $\Ob$ that have the highest frequencies with probability at least $1-\delta$.

\begin{enumerate}
\item $n\gets 0$. $\hat{f}_\D(A) \gets 0$ for any $A\in\Ob$.
\item $\Smp \gets \emptyset$.
\item If $n\geq |\D|$, $\Smp \gets \D$ and go to Step 8; otherwise, choose $\Delta n$ itemsets from $\D$ at uniformly random, and denote by $\Delta \Smp$ the chosen $\Delta n$ itemsets.
\item Label the transactions in $\Delta\Smp$: $\Delta\Smp = \{\tau_1,\cdots,\tau_{\Delta n}\}$.
\item For each $A\in\Ob$, compute $\hat{f}_\D(A) \gets \frac{1}{n+\Delta n}\left(n\cdot\hat{f}_\D(A)+\sum_{i=1}^{\Delta n}\phi_A(\tau_i)\right)$.
\item $n \gets n+\Delta n$. $\Smp \gets \Smp \cup \Delta \Smp$
\item $\epsilon \gets \sqrt{\frac{\log(2|\Ob|) + \log(1/\delta)}{2n}}$.
\item If $\epsilon < d$, where $d$ is the distance between the approximated frequencies $\hat{f}_\D(\cdot)$ of the $k$-th and $(k+1)$-th most frequent itemsets in $\Ob$, or $\Smp = \D$, then return the top-$k$ results (tie-breaking when necessary). Otherwise, go to Step 3.
\end{enumerate}
\end{minipage}
}

Note that in out top-$k$ approximation algorithm, the stopping condition depends on the frequency distance between the $k$-th and $(k+1)$-th most frequent itemsets. If these two frequencies tie, it is likely that many samples will  be needed since we cannot distinguish them based on approximated frequencies. Hence we require the number of samples should not exceed $|\D|$. If $|\D|$ samples are needed, we can compute the exact frequencies and apply tie-breaking rules on the ranked results.

%\bibliographystyle{./IEEEtran}
\bibliographystyle{plain}
\bibliography{./cs273}

\end{document}