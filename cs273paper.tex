\documentclass{article}

\usepackage{graphicx,tikz}
\usepackage{fullpage}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
%\usepackage{booktabs}
%\usepackage{color}

%\renewcommand{\baselinestretch}{0.95}
%\setlength{\textfloatsep}{0.1cm}
%\setlength{\abovecaptionskip}{0.1cm}

\begin{document}

\title{\Large\bf An Asymptotically Tighter Bound on Sampling for Frequent Itemsets Mining}
\author{Shiyu Ji\\ shiyu@cs.ucsb.edu}
\date{}
\maketitle

\newtheorem{definition}{Definition}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\theoremstyle{plain}
\newtheorem{lemma}{Lemma}
\theoremstyle{plain}
\newtheorem{corollary}{Corollary}
\theoremstyle{plain}

\begin{abstract}
In this paper we present a new error bound on sampling algorithms for frequent itemsets mining. We show that the new bound is asymptotically tighter than the state-of-art bounds, i.e., given the chosen samples, for small enough error probability, the new error bound is roughly half of the existing bounds.
\end{abstract}

\section{Introduction}
Frequent Itemsets (FI) mining has been popular in research recently \cite{AIS93, HCX07, RU15}. The goal of FI mining is to find out the items that most frequently appear in the observed transactions, e.g., the researchers who are the most prolific in writing papers with others, the patterns that appear frequently in a long piece of genetic code, etc. 
In the era of big data, to compute the exact frequencies can be very time consuming. Thus in many cases approximated values are also acceptable \cite{RU15}.
The state-of-art progressive sampling based FI approximation algorithms \cite{RU15} need an upper bound of the approximation error for the worst case, i.e., the maximum error the algorithm can generate among all the items. The algorithms keep taking new samples until the upper bound is less than the acceptable threshold. Hence how to bound the maximum error as tightly as possible is an interesting problem. The current bounds use some results of Rademacher average in statistical learning theory \cite{Vap98,Vap13,BBL04,BBL05}. 
However, we find that based on the idea given by \cite{BBL04}, we can develop a new upper bound \emph{without} Rademacher average. 
We also find that this new bound asymptotically outperforms the existing bounds, i.e., given the chosen samples, as the allowed error probability approaches zero, the new bound is roughly only half of the existing ones. This implies that by using the new bound, a progressive sampling based FI approximation algorithm can reach the guaranteed accuracy with much fewer samples. 


\section{Preliminaries}
\subsection{Frequency of Itemset}
\newcommand{\I}{\mathcal{I}}
\newcommand{\D}{\mathcal{D}}
In this paper we use the notations and definitions from Riondato and Upfal's pioneering work \cite{RU15}. 
Let $\I$ be the set of items. A transaction $\tau$ is a subset of $\I$ (i.e., $\tau \subseteq \I$).
An itemset $A$ is a set of items that appear together in a transaction $\tau$, i.e., $A \subseteq \tau$. Clearly any itemset is also a subset of $\I$. 
Let $\D$ be the set of all the transactions. In this paper we always assume $\D$ is a finite set. Denote by $T_\D(A)$ the set of all the transactions in $\D$ that contain the itemset $A$. $T_\D(A)$ is also known as the support set of $A$ in $\D$.
If $\D$ is a finite set, we can define the frequency of itemset $A$ in $\D$ as the fraction of transactions in $\D$ that contain $A$.
$$f_\D(A) = |T_\D(A)|/|\D|.$$
Clearly $0 \leq f_\D(A) \leq 1$ for any $A \subseteq \I$.

The goal of our sampling algorithm is to approximate $f_\D(A)$ given an itemset $A$ as accurately as possible.

\subsection{Approximation Algorithms}
\newcommand{\Smp}{\mathcal{S}}
An $(\epsilon,\delta)$-approximation algorithm of the frequencies $f_\D(\cdot)$ takes as input all the items $\I$ and outputs a sampled average $f_\Smp(A)$ for each $A\subseteq\I$ such that with probability at least $1-\delta$,
$$\max_{A\subseteq\I}|f_\D(A) - f_\Smp(A)| \leq \epsilon.$$
We often use progressive sampling \cite{RU15,RU16}, i.e., to keep taking more samples until a stopping condition is reached. A stopping condition usually takes the form $\Delta(n, \delta) \leq \epsilon$, where $n$ is the number of samples that have been taken, and $\Delta$ is an upper bound of the worst approximation error given by statistical learning theory. Note that $\Delta$ is usually a function of $n$ and $\delta$.

\subsection{Risk Bounds}
\label{sec:rb}
\newcommand{\R}{\mathcal{R}}
We briefly review some risk bounds in statistical learning theory \cite{BBL05} with the background of frequent itemsets mining. 

For each itemset $A\subseteq\I$, define the indicator function $\phi_A : 2^\I \to \{0, 1\}$ as follows.
$$\phi_A(\tau) = \begin{cases}
1 & \textrm{if $A\subseteq \tau$}\\
0 & \textrm{otherwise}\\
\end{cases},\quad
\tau\subseteq\I.$$
Clearly, the frequency $f_\D(A)$ equals to the \emph{true} average of $\phi_A(\tau)$ where $\tau$ goes over all the transactions in $\D$.
$$f_\D(A) = \frac{1}{|\D|} \sum_{\tau\in\D} \phi_A(\tau).$$
Similarly let $\Smp$ be the set of the sampled transactions. Then the \emph{sampled} average of $\phi_A(\tau)$ can be defined as
$$f_\Smp(A) = \frac{1}{|\Smp|} \sum_{\tau\in\Smp} \phi_A(\tau).$$
Clearly $f_\Smp(A)$ is the frequency of $A$ appearing in the sampled transactions $\Smp$.

Assume $|\Smp| = n$. For each transaction $\tau_i \in \Smp$, let $\sigma_i$ be a Rademacher random variable taking value from $\{-1, 1\}$ with uniform probability distribution. The $\sigma_i$'s are independent. Assuming $\I$ is finite, we define the sample conditional Rademacher average as follows.
$$\R_\Smp = \mathbb{E}_\sigma \left[\max_{A\subseteq\I}\frac{1}{n}\sum_{i=1}^n \sigma_i\phi_A(\tau_i)\right],$$
where $\mathbb{E}_\sigma$ denotes the expectation taken over all the random variables $\sigma_i$'s, conditionally on the sample $\Smp$. 

The following theorem tells us that Rademacher average can be used to upper bound the approximation error, even for the worst case.

\begin{theorem}
\label{thm:old}
(Theorem 3.2, \cite{BBL05}) For any $\delta>0$, with probability at least $1-\delta$,
$$\max_{A\subseteq\I} |f_\D(A) - f_\Smp(A)|\leq 2\R_\Smp + \sqrt{\frac{2\log(2/\delta)}{n}}.$$
\end{theorem}

If we want to use the upper bound given in Theorem \ref{thm:old} in an approximation algorithm, we still need to upper bound the $\R_\Smp$. A classical result is given by Massart \cite{Mas00}.

\begin{theorem}
\label{thm:massart}
(Lemma 5.2, \cite{Mas00}) Let $\ell = \max_{A\subseteq\I} [\sum_{i=1}^n\phi_A(\tau_i)^2]^{1/2}$ where each $\tau_i\in\Smp$. Then
$$\R_\Smp \leq \frac{\ell}{n}\sqrt{2\log N},$$
where $N = 2^{|\I|}$ and $n = |\Smp|$.
\end{theorem}
Hence we have the following stopping condition for an $(\epsilon,\delta)$-approximation sampling algorithm.
$$\Delta_1 := \frac{2\ell}{n}\sqrt{2\log N} + \sqrt{\frac{2\log(2/\delta)}{n}} \leq \epsilon.$$
However for many applications the above bound is not tight enough \cite{RU15,RU16}. In the next section we will first review the state-of-art bound on the worst approximation error, and then propose a new bound which seems tighter.

\section{Refining the Upper Bound}
\label{sec:refine}
The reason why the bound given in the previous section is often not tight enough in practice is that the $\ell$ defined in Theorem \ref{thm:massart} can be quite large. Suppose there is an itemset $A$ that almost always appears in every transaction in $\D$. Then no matter which sample the algorithm chooses, $\ell$ is roughly $\sqrt{n}$. For $\delta=0.01$, $N = 2^{1000}$, even 100,000 samples are taken, the upper bound is still larger than 0.15. For many applications such an upper bound cannot be acceptable and thus we need to take more samples. Clearly if the upper bound is tighter, a lot of samples can be saved.

Riondato and Upfal \cite{RU15} attempted to give a tighter bound of the Rademacher average $\R_\Smp$. 

\begin{theorem}
\label{thm:ru}
(Theorem 3, \cite{RU15}, revised) Let $w : \mathbb{R}^+ \to \mathbb{R}^+$ be the function defined as
$$w(s) = \frac{1}{s}\log \sum_{A\subseteq\I}\exp\left(\frac{s^2 \sum_{i=1}^n \phi_A(\tau_i)^2}{2n^2}\right).$$
Then $\R_\Smp \leq \min_{s>0} w(s)$.
\end{theorem}

{\bf Remark}. Note that in Theorem \ref{thm:ru}, the summation in $w(s)$ takes \emph{exactly} $2^{|\I|}$ terms. However in the original version in \cite{RU15}, the authors claimed that the summation could take much less than $2^{|\I|}$ terms. We argue that there is a gap between these two versions. Based on the proof given in \cite{RU15}, one can reach the inequality as follows.
\begin{equation}
\label{eqn:ru}\exp(s\R_\Smp) \leq \sum_{A\subseteq\I}\exp\left(\frac{s^2\sum_{i=1}^n \phi_A(\tau_i)^2}{2n^2}\right).
\end{equation}
Note that on the right hand side, each term in the summation is no less than 1. Hence when taking the logarithm on both sides and dividing by $s$, each of the $2^{|\I|}$ terms cannot be eliminated. Thus the range of the summation cannot be compressed.

Formally, suppose there is a set $\mathcal{V}\subseteq 2^\I$, where $2^\I$ denotes the power set of $\I$, such that
$$\alpha(s) := \sum_{A\in 2^\I}\exp\left(\frac{s^2\sum_{i=1}^n \phi_A(\tau_i)^2}{2n^2}\right) \leq \sum_{A\in \mathcal{V}}\exp\left(\frac{s^2\sum_{i=1}^n \phi_A(\tau_i)^2}{2n^2}\right) :=\beta(s).$$
We take the limits as $s$ approaches 0.
$$2^{|\I|}=\lim_{s\to 0}\alpha(s) \leq \lim_{s\to 0}\beta(s) = |\mathcal{V}|.$$
Hence $\mathcal{V} = 2^\I$. This implies any summation over only a part of $2^\I$ must be less than the summation over all of $2^\I$. Thus one cannot use Inequality (\ref{eqn:ru}) to reach Theorem 3 in \cite{RU15}.

\subsection{New Bound Without Rademacher Average}
In statistical learning theory, the upper bound given by Theorem \ref{thm:old} is for the general case, i.e., the set of transactions $\D$ can be infinite or finite. However, for frequent itemsets mining, the transactional data set $\D$ is always finite. Given the assumption that $\D$ is finite, can we establish any upper bound without using the Rademacher average? Following the similar lines given by Boucheron, Bousquet and Lugosi \cite{BBL04}, we can give an affirmative answer.

For any $\epsilon > 0$,
$$\begin{aligned}
& \Pr[\max_{A\subseteq\I}|f_\D(A)-f_\Smp(A)|>\epsilon] \\
= & \Pr[\exists A\subseteq\I, f_\D(A)-f_\Smp(A)>\epsilon \vee f_\D(A)-f_\Smp(A)<-\epsilon] \\
\leq & \Pr[\exists A\subseteq\I, f_\D(A)-f_\Smp(A)>\epsilon] +\Pr[\exists A\subseteq\I, f_\D(A)-f_\Smp(A)<-\epsilon] && \textrm{(union bound)}\\
\leq & \sum_{A\subseteq\I} \Pr[f_\D(A)-f_\Smp(A)>\epsilon] + \sum_{A\subseteq\I} \Pr[f_\Smp(A)-f_\D(A)>\epsilon] && \textrm{(union bound)}.
\end{aligned}$$
Recall Hoeffding's inequalities \cite{H63}. Let $X_1, \cdots, X_n$ be independent random variables bounded by the intervals $[a_i, b_i]$. Define the sampled average of them as
$$\overline{X} = \frac{1}{n}\sum_{i=1}^n X_i.$$
Then for any $t>0$,
$$\Pr[\overline{X} - \mathbb{E}[\overline{X}] > t] \leq \exp\left(-\frac{2n^2t^2}{\sum_{i=1}^n (b_i-a_i)^2}\right),$$
and
$$\Pr[\mathbb{E}[\overline{X}] - \overline{X} > t] \leq \exp\left(-\frac{2n^2t^2}{\sum_{i=1}^n (b_i-a_i)^2}\right).$$
Note that if we set $X_i = \phi_A(\tau_i)$, then $X_i$'s are independent since $\tau_i$'s are independent, and thus $f_\D(A) = \mathbb{E}[\overline{X}]$ and $f_\Smp(A) = \overline{X}$. Based on Hoeffding's inequalities, 
$$\Pr[f_\D(A) - f_\Smp(A) > \epsilon] \leq \exp\left(-2n\epsilon^2\right),$$
and
$$\Pr[f_\Smp(A) - f_\D(A) > \epsilon] \leq \exp\left(-2n\epsilon^2\right).$$
Putting the above results together, we have
$$\begin{aligned}
& \Pr[\max_{A\subseteq\I}|f_\D(A)-f_\Smp(A)|>\epsilon] \\
\leq & 2\sum_{A\subseteq\I} \exp\left(-2n\epsilon^2\right) \\
= & 2N \exp\left(-2n\epsilon^2\right),
\end{aligned}$$
where $N = 2^{|\I|}$. 
Equivalently for any $\delta>0$, with probability at least $1-\delta$,
$$|f_\D(A)-f_\Smp(A)| \leq \sqrt{\frac{\log(2N) + \log(1/\delta)}{2n}} =: \Delta_2.$$
Note that the above bound $\Delta_2$ is very similar to the result in Section 3.4, \cite{BBL04}. Now the new bound $\Delta_2$ can generate a new stopping condition for an approximation algorithm.

Recall the classical upper bound given in Section \ref{sec:rb}.
$$\Delta_1 := \frac{2\ell}{n}\sqrt{2\log N} + \sqrt{\frac{2\log(2/\delta)}{n}}.$$
Clearly $\lim_{\delta\to 0}\Delta_1/\Delta_2 = 2$, i.e., when $\delta$ is very small, the bound $\Delta_1$ is roughly twice of $\Delta_2$ given the sample size $n$.
This assures us that the new bound $\Delta_2$ is highly competitive.

Theorem \ref{thm:ru} can give another upper bound on the worst approximation error. However, since the number of terms in the summation grows exponentially on $|\I|$, to find the minimum is computationally infeasible. Furthermore, even if the minimum $w(s^*)$ is found, let $\Delta_1'$ be the upper bound of this variant defined as 
$$\Delta_1' := w(s^*) + \sqrt{\frac{2\log(2/\delta)}{n}}.$$
By fixing the sample $\Smp$, we still have $\lim_{\delta\to 0}\Delta_1'/\Delta_2 = 2$. For small $\delta$, the new bound without Rademacher average still outperforms the existing ones.

%\bibliographystyle{./IEEEtran}
\bibliographystyle{plain}
\bibliography{./cs273}

\end{document}